# -*- coding: utf-8 -*-
"""Computational Stat ASSIGN 1.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qjG1A7gCeC2asBshZGzP-TYp7ZDKHPPc

**ASSIGNMENT-1 COMPUTATIONAL STATISTICS**

Done by :Blesson Issac and Michael Bimal

1. Suppose that you have only have access to a routine that produces samples U from the Uniform[0, 1]
distribution. Most packages used for statistical computing, such as R or MATLAB have access
to such a routine.

(a) Write a program that uses the inversion method to sample from a Binomial(10, 1/3) distribution.
Run the program and plot a histogram of 1000 samples. Recall that here you will
need to make use of the pseudo-inverse of the cumulative distribution function.
"""

import numpy as np
import matplotlib.pyplot as plt
import math

# Function For binomial PMF
def binomial_pmf(n, p, k):
    return math.comb(n, k) * (p ** k) * ((1 - p) ** (n - k))

# Function For CDF from the PMF
def binomial_cdf(n, p):
    pmf_vals = [binomial_pmf(n, p, k) for k in range(n+1)]
    cdf_val = np.cumsum(pmf_vals)
    return cdf_val

cdf_vals = binomial_cdf(n, p)

# Function to sample from the Binomial(n, p) distribution using the inversion method
def sample_from_binomial(n, p, cdf_vals):
    u = np.random.uniform(0, 1)  # gives a uniform random variable between 0 and 1
    for k, cdf_val in enumerate(cdf_vals):
        if u < cdf_val:
            return k  # Return the binomial sample


# Parameters for the Binomial distribution
n = 10  # Number of trials
p = 1/3  # Probability of success

# Generate 1000 samples from the Binomial(10, 1/3) distribution
samples = [sample_from_binomial(n, p, cdf_vals) for _ in range(1000)]

# Plot the histogram of the samples
plt.hist(samples, bins=np.arange(-0.5, n+1.5, 1), edgecolor='black', density=True)
plt.title("Distribution of random numbers from a binomial distribution")
plt.xlabel("Number of Successes")
plt.ylabel("Frequency")
plt.xticks(range(n+1))  # Set x-ticks to integer values
plt.show()

"""The inversion method is used for sampling from a probability distribution using uniformly distributed random numbers. This method relies on the principle that if you have a uniform random variable
U distributed over
[0,1], then the variable X=F
−1
 (U) will have a distribution function
F, where
F is the cumulative distribution function (CDF) of the desired distribution.

(b) Now, using what you know about Binomial distributions, write a program that uses a combination
of the inversion and transformation methods to sample from the same Binomial(10, 1/3)
distribution. Run the program and plot a histogram of 1, 000 samples.
For this part, your answer should include a short proof of the correctness of your approach
as well as the program itself.
"""

import numpy as np
import matplotlib.pyplot as plt

# Parameters
n = 10  # number of trials
p = 1/3  # probability of success
num_samples = 1000  # number of samples to generate

# Sampling function
def sample_binomial(n, p, num_samples):
    # Generate uniform random samples
    u = np.random.uniform(0, 1, (num_samples, n)) # u is the uniform samples
    # Convert to Bernoulli trials
    bernoulli_trials = (u <= p).astype(int)
    # Sum the Bernoulli trials to get Binomial samples
    binomial_samples = np.sum(bernoulli_trials, axis=1)
    return binomial_samples

# Generate samples
samples = sample_binomial(n, p, num_samples)

# Plot histogram
plt.hist(samples, bins=np.arange(-0.5, n+1.5, 1), edgecolor='black', density=True)
plt.title('Distribution of random numbers from Binomial(10, 1/3)')
plt.xlabel('Number of Successes')
plt.ylabel('Frequency')
plt.xticks(range(n+1))
plt.show()

"""(c) We know that the expectation of a Binomial(10, 1/3) distribution is exactly 10/3. However,
let’s pretend that we don’t know its true value and want to estimate it using elementary
Monte Carlo integration.
Use 100 samples generated with one of the methods above to estimate the expectation
of a Binomial(10, 1/3) distribution and provide the standard error of the estimate and
approximate confidence bounds as per the Central Limit Theorem. You should briefly
restate the theory behind this approximation before applying it: simply putting the answer
with confidence bounds will not earn full marks.
"""

import numpy as np
import matplotlib.pyplot as plt

# Parameters
n = 10  # number of trials
p = 1/3  # probability of success
num_samples = 100  # number of samples to generate

# Sampling function using the transformation method
def sample_binomial(n, p, num_samples):
    uniform_samples = np.random.uniform(0, 1, (num_samples, n))
    bernoulli_trials = (uniform_samples <= p).astype(int)
    binomial_samples = np.sum(bernoulli_trials, axis=1)
    return binomial_samples


# Generate samples
samples = sample_binomial(n, p, num_samples)

# Calculate sample mean and standard deviation
sample_mean = np.mean(samples)
sample_std = np.std(samples, ddof=1)  # ddof=1 for sample standard deviation

# Calculate standard error
standard_error = sample_std / np.sqrt(num_samples)

# Calculate confidence interval (95%)
confidence_level = 1.96  # z-score for 95% confidence
lower_bound = sample_mean - confidence_level * standard_error
upper_bound = sample_mean + confidence_level * standard_error

# Output results
print(f"Estimated Expectation (Mean): {sample_mean}")
print(f"Standard Error: {standard_error}")
print(f"95% Confidence Interval: ({lower_bound}, {upper_bound})")

"""2) Write a program that draws samples of Poisson(t) random variables using Uniform[0, 1] variables
as input. Use the transformation method to do so.
Hint: recall that if Xi are i.i.d. Exponential(1) random variables and Sn =
Pn
i=1 Xi, then
P(Sn ≤ t ≤ Sn+1) = e−t tn
n! . You may use this result without proof.
Run the program with t = 1 and plot a histogram of 1000 samples. Use 10, 100, 1000 and 10000
samples to estimate the mean of the Poisson(1) distribution and provide the standard error and
associated confidence bounds on the estimate for each case.
"""

import numpy as np
import matplotlib.pyplot as plt

# Function to sample from Poisson(t) distribution
def sample_poisson(t, num_samples):
    poisson_samples = []
    for _ in range(num_samples):
        s = 0  # the time interval for one exponential random sample to occur
        n = 0  # event count
        while s < t:
            # Generate an Exponential(1) sample
            u = np.random.uniform(0, 1)
            s += -np.log(u)  # Inverse transform method for Exponential(1)
            n += 1
        poisson_samples.append(n - 1)  # n - 1 because we count how many we had before exceeding t
    return np.array(poisson_samples)

# Parameters
t = 1
sample_sizes = [10, 100, 1000, 10000]

# Data storage for results
results = {}

# Sampling and calculating statistics
for size in sample_sizes:
    samples = sample_poisson(t, size)
    mean_estimate = np.mean(samples)
    std_dev = np.std(samples, ddof=1)  # Sample standard deviation
    standard_error = std_dev / np.sqrt(size)

    # 95% confidence interval
    confidence_level = 1.96
    lower_bound = mean_estimate - confidence_level * standard_error
    upper_bound = mean_estimate + confidence_level * standard_error

    # Store results
    results[size] = {
        'mean': mean_estimate,
        'std_error': standard_error,
        'confidence_interval': (lower_bound, upper_bound)
    }

    # Plotting histogram for 1000 samples
    if size == 1000:
        plt.hist(samples, bins=np.arange(-0.5, max(samples)+1.5, 1), density=True, alpha=0.75, color='blue', edgecolor='black')
        plt.title('Histogram of 1000 Samples from Poisson(1)')
        plt.xlabel('Number of Events')
        plt.ylabel('Density')
        plt.xticks(range(max(samples)+1))
        plt.grid(axis='y', alpha=0.75)
        plt.show()

# Print results
for size, stats in results.items():
    print(f"Sample Size: {size}")
    print(f"Estimated Mean: {stats['mean']}")
    print(f"Standard Error: {stats['std_error']}")
    print(f"95% Confidence Interval: {stats['confidence_interval']}\n")

"""3. Finally, let us consider rejection sampling. We saw in class that rejection sampling is a generalpurpose
sampling method that needs to be used judiciously in order to make it work efficiently.
A poorly chosen proposal distribution will result in a high rejection rate.

(a) Suppose we have a mixture of a N(1, 0.5) and a N(2, 0.1) distribution with weights α1 = 0.2
and α2 = 0.8. Use a rejection sampling method to draw from this mixture with a single
normal distribution as a proposal. Choose the best parameters for this distribution (with
some justification why you have done so). What is the acceptance rate of your rejection
sampler? As a sanity check, you can try implementing the composition method to check if
you are getting the right answer.
"""

import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

# Target distribution: mixture of two normal distributions
alpha1, alpha2 = 0.2, 0.8
mean1, std1 = 1, 0.5  # N(1, 0.5)
mean2, std2 = 2, 0.1  # N(2, 0.1)

# Define the target distribution (mixture of normals)
def target_distribution(x):
    return alpha1 * stats.norm.pdf(x, mean1, std1) + alpha2 * stats.norm.pdf(x, mean2, std2)

# Proposal distribution: N(2, 0.5)
proposal_mean = 2
proposal_std = 0.5
proposal = stats.norm(loc=proposal_mean, scale=proposal_std)

# Rejection sampling function
def rejection_sampling(n_samples):
    samples = []
    M = 3  # Upper bound for scaling factor (should be fine-tuned for efficiency)
    while len(samples) < n_samples:
        x_proposed = proposal.rvs()  # Sample from proposal distribution
        u = np.random.uniform(0, 1)  # Uniformly distributed value for rejection criterion

        # Accept or reject the sample
        if u < target_distribution(x_proposed) / (M * proposal.pdf(x_proposed)):
            samples.append(x_proposed)

    return np.array(samples)

# Generate samples using rejection sampling
n_samples = 10000
samples = rejection_sampling(n_samples)

# Plot the result
x = np.linspace(-1, 4, 1000)
plt.figure(figsize=(10, 6))
plt.hist(samples, bins=50, density=True, alpha=0.6, color='g', label="Rejection Samples")
plt.plot(x, target_distribution(x), 'r-', lw=2, label="Target Distribution (Mixture)")
plt.plot(x, proposal.pdf(x), 'b--', lw=2, label="Proposal Distribution (N(2, 0.5))")
plt.title("Rejection Sampling for a Mixture of Two Normals")
plt.legend()
plt.show()

# Estimate acceptance rate
def estimate_acceptance_rate(n_trials=1000):
    accepted = 0
    M = 3
    for _ in range(n_trials):
        x_proposed = proposal.rvs()
        u = np.random.uniform(0, 1)
        if u < target_distribution(x_proposed) / (M * proposal.pdf(x_proposed)):
            accepted += 1
    return accepted / n_trials

acceptance_rate = estimate_acceptance_rate()
print(f"Estimated Acceptance Rate: {acceptance_rate * 100:.2f}%")

"""An acceptance rate of 30.20% is considered relatively efficient for this method, given the straightforward nature of rejection sampling. Nonetheless, further optimization of the proposal distribution might enhance this rate.

SANITY CHECK USING COMPOSITION METHOD
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters for the mixture of normals
mu1, sigma1, weight1 = 1, np.sqrt(0.5), 0.2  # N(1, 0.5)
mu2, sigma2, weight2 = 2, np.sqrt(0.1), 0.8  # N(2, 0.1)

# Number of samples to generate
n_samples = 10000

# Generate samples using the composition method
samples = []
for _ in range(n_samples):
    # Choose which distribution to sample from based on weights
    if np.random.rand() < weight1:
        sample = np.random.normal(mu1, sigma1)
    else:
        sample = np.random.normal(mu2, sigma2)
    samples.append(sample)

# Convert to numpy array for easier handling
samples = np.array(samples)

# Plotting the results
plt.figure(figsize=(10, 6))
sns.histplot(samples, bins=50, kde=True, stat="density", color='skyblue', label='Sampled Mixture')
plt.title('Samples from Mixture of Normals')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()

"""This graph serves as a sanity check, demonstrating that the composition method has successfully captured the essential characteristics of the mixture of normal distributions. Assuming the underlying distributions are correctly centered around these values with suitable variability, the graph indicates that both the sampling and histogram plotting have been properly conducted."""